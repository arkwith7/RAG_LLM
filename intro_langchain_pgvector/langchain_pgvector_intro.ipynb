{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain and pgvector: Up and Running\n",
    "[LangChain](https://langchain.com) is the most popular framework for building LLM applications and agents. This notebook is an introduction to building LLM applications with the LangChain framework, using PostgreSQL and pgvector as a vector database for embeddings data.\n",
    "\n",
    "We'll use the example of creating a chatbot to answer questions about the blog posts from the Timescale blog to illustrate the following concepts:\n",
    "- How to prepare your documents for insertion into PostgreSQL and pgvector using LangChain document transformer TextSplitter\n",
    "- How to create embeddings from your data using the OpenAI embeddings model and insert them into PostgreSQL and pgvector.\n",
    "- How to use embeddings retrieved from a vector database to augment LLM generation. \n",
    "\n",
    "This is a great first step for more advanced LangChain projects in Python -- for example, creating a chatbot for your company documentation or an application to answer questions from uploaded PDFs.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "- Signup for an OpenAI Developer Account and create an API Key. See [OpenAI's developer platform](https://platform.openai.com/overview).\n",
    "- Install Python\n",
    "- Install and configure a python virtual environment. We recommend [Pyenv](https://github.com/pyenv/pyenv)\n",
    "- Install the requirements for this notebook using the following command:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Or if you already have langchain installed, run: \n",
    "```pip install --upgrade langchain```\n",
    "\n",
    "Protip: Add your OpenAI API Key to the provided shell script (named ```setenv.sh```) and run it to set your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Run export OPENAI_API_KEY=sk-YOUR_OPENAI_API_KEY...\n",
    "# Get openAI api key by reading local .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY  = os.environ['OPENAI_API_KEY']\n",
    "# print(OPENAI_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a way for LangChain to interact with PostgreSQL and pgvector. This is acheived by importing the PGVector class from the langchain.vectorstores package as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import PGVector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll construct our connection string for LangChain to connect to our PostgreSQL database.\n",
    "\n",
    " Because LangChain uses SQLAlchemy to connect to SQL databases like PostgreSQL, we need to create our connection string programmatically, reading each of the components of the string (host, database name, password, port etc) from our environment variables.\n",
    "\n",
    "In this example, we'll use a PostgreSQL database with pgvector installed that's hosted on Timescale. You can create your own cloud PostgreSQL database in minutes [at this link](https://console.cloud.timescale.com/signup) to follow along. If you're using a Timescale database, you can find all this information in the \"Cheat Sheet\" file you download when you first create your new database service.\n",
    "\n",
    "Alternatively, you can also use a local PostgreSQL database if you prefer.\n",
    "\n",
    "Protip: Add your database credentials to the provided shell script (named ```setenv.sh```)  and run it to set your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the PGVector Connection String from params\n",
    "\n",
    "# Found in the credential cheat-sheet or \"Connection Info\" in the Timescale console\n",
    "# In terminal, run: export VAR_NAME=value for each of the values below\n",
    "host= os.environ['TIMESCALE_HOST']\n",
    "port= os.environ['TIMESCALE_PORT']\n",
    "user= os.environ['TIMESCALE_USER']\n",
    "password= os.environ['TIMESCALE_PASSWORD']\n",
    "dbname= os.environ['TIMESCALE_DBNAME']\n",
    "\n",
    "# We use postgresql rather than postgres in the conn string since LangChain uses sqlalchemy under the hood\n",
    "# You can remove the ?sslmode=require if you have a local PostgreSQL instance running without SSL\n",
    "# CONNECTION_STRING = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}?sslmode=require\"\n",
    "CONNECTION_STRING = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}?sslmode=disable\"\n",
    "# print(CONNECTION_STRING)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Split a CSV file into smaller chunks while preserving associated metadata\n",
    "In this section, we will parse our CSV file into smaller chunks for similarity search and retrieval, with help from LangChains TokenTextSplitter.\n",
    "\n",
    "First let's take a look at the CSV file we'll be working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
       "      <td>This is an installment of our “Community Membe...</td>\n",
       "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CloudQuery on Using PostgreSQL for Cloud Asset...</td>\n",
       "      <td>This is an installment of our “Community Membe...</td>\n",
       "      <td>https://www.timescale.com/blog/cloudquery-on-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How a Data Scientist Is Building a Time-Series...</td>\n",
       "      <td>This is an installment of our “Community Membe...</td>\n",
       "      <td>https://www.timescale.com/blog/how-a-data-scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Conserv Safeguards History: Building an En...</td>\n",
       "      <td>This is an installment of our “Community Membe...</td>\n",
       "      <td>https://www.timescale.com/blog/how-conserv-saf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How Messari Uses Data to Open the Cryptoeconom...</td>\n",
       "      <td>This is an installment of our “Community Membe...</td>\n",
       "      <td>https://www.timescale.com/blog/how-messari-use...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  How to Build a Weather Station With Elixir, Ne...   \n",
       "1  CloudQuery on Using PostgreSQL for Cloud Asset...   \n",
       "2  How a Data Scientist Is Building a Time-Series...   \n",
       "3  How Conserv Safeguards History: Building an En...   \n",
       "4  How Messari Uses Data to Open the Cryptoeconom...   \n",
       "\n",
       "                                             content  \\\n",
       "0  This is an installment of our “Community Membe...   \n",
       "1  This is an installment of our “Community Membe...   \n",
       "2  This is an installment of our “Community Membe...   \n",
       "3  This is an installment of our “Community Membe...   \n",
       "4  This is an installment of our “Community Membe...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.timescale.com/blog/how-to-build-a-...  \n",
       "1  https://www.timescale.com/blog/cloudquery-on-u...  \n",
       "2  https://www.timescale.com/blog/how-a-data-scie...  \n",
       "3  https://www.timescale.com/blog/how-conserv-saf...  \n",
       "4  https://www.timescale.com/blog/how-messari-use...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('blog_posts_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, this is a CSV file of blog posts about Timescale use cases. \n",
    "\n",
    "Ordinarily, we would use the langchain [CSVLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/csv) to load the contents of a CSV file, but in this case, we need to pre-process the content column of our CSV to be able to create embeddings for each blog post within the token limits of the OpenAI embeddings API.\n",
    "\n",
    "We also need a way to split the text of content column of the CSV while retaining the associated metadata with that text (i.e the blog title and URL)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "We'll use LangChain's [Token Text Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token) to help us split up the content column of our CSV into chunks of a specified token amount. You an alternatively use the [Recursive Character Text Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter), if you'd rather split text by number of characters rather than tokens.\n",
    "\n",
    "We will split the text into chunks of around 512 tokens, with a 20% or 103 token overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# We need to split the text into chunks of 512 tokens, with 20% token overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=512,chunk_overlap=103)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper func: calculate number of tokens\n",
    "def num_tokens_from_string(string: str, encoding_name = \"cl100k_base\") -> int:\n",
    "    if not string:\n",
    "        return 0\n",
    "    # Returns the number of tokens in a text string\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list for smaller chunked text and metadata\n",
    "new_list = []\n",
    "# Create a new list by splitting up text into token sizes of around 512 tokens\n",
    "for i in range(len(df.index)):\n",
    "    text = df['content'][i]\n",
    "    token_len = num_tokens_from_string(text)\n",
    "    if token_len <= 512:\n",
    "        new_list.append([df['title'][i], df['content'][i], df['url'][i]])\n",
    "    else:\n",
    "        #split text into 512 token chunks using text splitter\n",
    "        split_text = text_splitter.split_text(text)\n",
    "        for j in range(len(split_text)):\n",
    "            new_list.append([df['title'][i], split_text[j], df['url'][i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the content looks after being split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
       "      <td>This is an installment of our “Community Membe...</td>\n",
       "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
       "      <td>.One of the motivating factors for this book w...</td>\n",
       "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
       "      <td>showing various graphs for various weather da...</td>\n",
       "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
       "      <td>Some of the articles that helped us get start...</td>\n",
       "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CloudQuery on Using PostgreSQL for Cloud Asset...</td>\n",
       "      <td>This is an installment of our “Community Membe...</td>\n",
       "      <td>https://www.timescale.com/blog/cloudquery-on-u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  How to Build a Weather Station With Elixir, Ne...   \n",
       "1  How to Build a Weather Station With Elixir, Ne...   \n",
       "2  How to Build a Weather Station With Elixir, Ne...   \n",
       "3  How to Build a Weather Station With Elixir, Ne...   \n",
       "4  CloudQuery on Using PostgreSQL for Cloud Asset...   \n",
       "\n",
       "                                             content  \\\n",
       "0  This is an installment of our “Community Membe...   \n",
       "1  .One of the motivating factors for this book w...   \n",
       "2   showing various graphs for various weather da...   \n",
       "3   Some of the articles that helped us get start...   \n",
       "4  This is an installment of our “Community Membe...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.timescale.com/blog/how-to-build-a-...  \n",
       "1  https://www.timescale.com/blog/how-to-build-a-...  \n",
       "2  https://www.timescale.com/blog/how-to-build-a-...  \n",
       "3  https://www.timescale.com/blog/how-to-build-a-...  \n",
       "4  https://www.timescale.com/blog/cloudquery-on-u...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.DataFrame(new_list, columns=['title', 'content', 'url'])\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "#Quick check on how many items in our new list\n",
    "print(len(new_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: save to new csv for easy reloading\n",
    "df_new.to_csv('blog_posts_data_chunked.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Insert embeddings into PostgreSQL and pgvector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our original CSV split up into smaller chunks and the associated metadata preserved, we will use the LangChain [Pandas Data Frame Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/pandas_dataframe) to load data from our new pandas data frame and insert it into our PostgreSQL database with pgvector installed.\n",
    "\n",
    "Note that we must specify which column in the Data Frame contains the text that we'll create embeddings for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load documents from Pandas dataframe for insertion into database\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "# page_content_column is the column name in the dataframe that contains the we'll create embeddings for\n",
    "loader = DataFrameLoader(df_new, page_content_column = 'content')\n",
    "docs = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the OpenAI embeddings model for our documents, so let's import the OpenAIEmbeddings module from the langchain.embeddings package and create an instance of it. \n",
    "\n",
    "This instance can be used to generate embeddings for text data using the OpenAI API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/Dev/RAG_LLM/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create embeddings for all the data in our DataFrame, let's briefly overview how creating an embedding works. \n",
    "\n",
    "Here's how we create an embedding for a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.004347408032927978, 0.0003213765332503933, -0.009069139740747278, -0.03990158666707763, -0.03149572273275746]\n"
     ]
    }
   ],
   "source": [
    "# Create OpenAI embedding using LangChain's OpenAIEmbeddings class\n",
    "query_string = \"PostgreSQL is my favorite database\"\n",
    "embed = embeddings.embed_query(query_string)\n",
    "print(len(embed)) # Should be 1536, the dimensinality of the OpenAI model's embeddings\n",
    "print(embed[:5]) # Should be a list of floats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the main event, we'll connect to our PostgreSQL database and store the documents we loaded along with their embeddings.\n",
    "\n",
    "Thanks to LangChain, creating the embeddings and storing the data in our PostgreSQL database is a one command operation!\n",
    "\n",
    "We pass in the following arguments:\n",
    "- ```documents```: The documents we loaded from the Pandas Data Frame.\n",
    "- ```embedding```: Our instance of the OpenAI embeddings class, which is the model we'll use the create the embeddings. \n",
    "-  ```collection_name```: The name of the table we want our embeddings and metadata to live in\n",
    "- ```distance_strategy```: The distance strategy we wan to use to calculate the distance between vectors, in our case we'll use cosine distance\n",
    "- ```connection_string```: The connection string to our PostgreSQL database which we constructed in the setup section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/Dev/RAG_LLM/venv/lib/python3.11/site-packages/langchain_community/vectorstores/pgvector.py:322: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create a PGVector instance to house the documents and embeddings\n",
    "from langchain.vectorstores.pgvector import DistanceStrategy\n",
    "db = PGVector.from_documents(\n",
    "    documents= docs,\n",
    "    embedding = embeddings,\n",
    "    collection_name= \"blog_posts\",\n",
    "    distance_strategy = DistanceStrategy.COSINE,\n",
    "    connection_string=CONNECTION_STRING)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is in the database, let's perform a similarity search to fetch the documents most similar to a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Query for which we want to find semantically similar documents\n",
    "query = \"Tell me about how Edeva uses Timescale?\"\n",
    "\n",
    "#Fetch the k=3 most similar documents\n",
    "docs =  db.similarity_search(query, k=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query on our database returns a list of LangChain Documents, let's learn how to interact with those documents below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content snippet: map applications.If you are planning to store time-series data, Timescale is the way to go. It makes it easy to get started because it is “just” SQL, and at the same time, you get the important features needed to work with time-series data. I recommend you have a look, especially at continuous aggregations.Think about the whole lifecycle when you start. Will your use cases allow you to use features like compression, or do you need to think about how to store long-term data outside of TimescaleD\n",
      "Document title: How Edeva Uses Continuous Aggregations and IoT to Build Smarter Cities\n",
      "Document url: https://www.timescale.com/blog/how-edeva-uses-continuous-aggregations-and-iot-to-build-smarter-cities/\n"
     ]
    }
   ],
   "source": [
    "# Interact with a document returned from the similarity search on pgvector\n",
    "doc = docs[0]\n",
    "\n",
    "# Access the document's content\n",
    "doc_content = doc.page_content\n",
    "# Access the document's metadata object\n",
    "doc_metadata = doc.metadata\n",
    "\n",
    "print(\"Content snippet:\" + doc_content[:500])\n",
    "print(\"Document title: \" + doc_metadata['title'])\n",
    "print(\"Document url: \" + doc_metadata['url'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Question Answering with Retrieval Augmented Generation\n",
    "Next let's tie everything we've learned together and build a simple example of using LangChain for questions answering using an LLM from OpenAI and the most relevant documents the question from our database. \n",
    "\n",
    "This technique is called Retrieval Augmented Generation and works as follows:\n",
    "- Create an embedding vector for the user question.\n",
    "- Use pgvector to perform a vector similarity search and retrieve the k nearest neighbors to the question embedding from our database of embedding vectors representing the blog content. In our example, we’ll use k=3, finding the three most similar embedding vectors and associated content.\n",
    "- Supply the content retrieved from the database as additional context to the model and ask it to perform a completion task to answer the user question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To more easily retrieve documents from our PostgreSQL vector database, we'll use a LangChain [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/). \n",
    "\n",
    "In LangChain, a retriever is an interface that returns documents given an unstructured query. A retriever's main purpose is only to return (or retrieve) documents. \n",
    "\n",
    "We will use a [vector store-backed retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/vectorstore) which is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the Vector Store class to make it conform to the Retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from database\n",
    "# We specify the number of results we want to retrieve (k=3)\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\": 3}\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the LLM we want to use to generate a response to our question. In this case we'll use [OpenAI's GPT-3.5 model](https://platform.openai.com/docs/models/) with a 16k token context window, so that we won't have any trouble fitting in retrieved documents as context in addition to the user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/Dev/RAG_LLM/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature = 0.0, model = 'gpt-3.5-turbo-16k')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we'll use one of the most useful chain's in LangChain, the [Retrieval Q+A chain](https://python.langchain.com/docs/modules/chains/popular/vector_db_qa), which is used for question answering over an a vector database (vector store or index as its also known.)\n",
    "\n",
    "\n",
    "We'll combine it with a [stuff chain](https://python.langchain.com/docs/modules/chains/document/stuff) which takes a list of documents, inserts them all into a prompt (_stuffs_ them in) and passes that prompt to an LLM.\n",
    "\n",
    "And for the final ingredient, let's formulate a question we want to the model to answer with the help from the documents in our database and pass it to our chain to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/Dev/RAG_LLM/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Edeva uses continuous aggregates in their smart city platform, EdevaLive. They collect large amounts of data from IoT devices, including traffic flow data from their dynamic speed bump called Actibump. Continuous aggregates allow them to roll up multiple resolutions of their sensor account data and people count data, making it available in a more efficient way. This helps them analyze and visualize the data faster, enabling them to provide valuable remote monitoring services and statistics to their customers. They also use continuous aggregates to roll up high-resolution data to lower resolutions, optimizing their data processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "query =  \"How does Edeva use continuous aggregates?\"\n",
    "response = qa_stuff.run(query)\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Cite your sources with LangChain and pgvector for RAG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even more advanced functionality, you might want your answer to include the sources used to give users peace of mind. Here's how you can do that with the RetrievalQA chain using the ```return_source_documents``` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/Dev/RAG_LLM/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# New chain to return context and sources\n",
    "qa_stuff_with_sources = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "query =  \"How does Edeva use continuous aggregates?\"\n",
    "\n",
    "# To run the query, we use a different syntax since we're returning more than just the response text\n",
    "responses = qa_stuff_with_sources({\"query\": query})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the source documents that got returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is an installment of our “Community Member Spotlight” series, where we invite our customers to share their work, shining a light on their success and inspiring others with new ways to use technology to solve problems.In this edition, John Eskilsson, software architect at Edeva, shares how his team collects huge amounts of data (mainly) from IoT devices to help build safer, smarter cities and leverages continuous aggregations for lightning-fast dashboards.Founded in 2009 in Linköping,Edevais a Swedish company that creates powerful solutions for smart cities. It offers managed services and complete systems, including hardware and software platforms.As the creators of the dynamic speed bumpActibumpand the smart city platformEdevaLive, the Edeva team works mainly for municipal, regional, and national road administrations, toll stations, environmental agencies, and law enforcement agencies.The team also solves many other problems, from obtaining large amounts of environmental data for decision-making to developing a screening scale to help law enforcement agencies assess vehicle overloading. The latter, for instance, decreased the amount of time needed to control each vehicle, speeding up traffic checks and allowing law enforcement agencies to control more vehicles.About the TeamTheteam at Edevais a small but impactful group of 11 working on everything from creating hardware IoT devices to analyzing time-series data and making it accessible to customers—and, sometimes—the public.As a software architect, I am in charge of building the best possible solution to receive, store, analyze, visualize, and share the customers’ event data. Our team then comes together to create solutions that work and that the customer actually wants.About the ProjectEdeva has created a dynamic speed bump calledActibumpand the smart city platformEdevaLive.The Actibump has been used in Sweden since 2010. Speeding vehicles activate a hatch in the road that lowers a few centimeters, creating an inverted speed bump. Providing good accessibility for public transportation, such as buses and emergency vehicles, the Actibump still ensures a safe speed for pedestrians and other vulnerable road users. It is also an environmentally friendly solution, helping decrease noise and emissions.The Actibump can be combined with the EdevaLive system, delivering valuable remote monitoring services and statistics to Edeva’s customers.Most of the data we collect is based on IoT devices:Traffic flow data: The Actibump measures the speed of oncoming traffic to decide if it needs to activate the speed bump or not.', metadata={'title': 'How Edeva Uses Continuous Aggregations and IoT to Build Smarter Cities', 'url': 'https://www.timescale.com/blog/how-edeva-uses-continuous-aggregations-and-iot-to-build-smarter-cities/'}),\n",
       " Document(page_content=\"re being more responsible users of TimescaleDB. We’re at that sweet spot within TimescaleDB where it’s found product-market fit within Density.Brock:It’s hard to name my favorite TimescaleDB feature, but continuous aggregates have been a game-changer in a variety of different ways. Early on, whenever we first onboarded to TimescaleDB and deployed, it made development significantly faster, and we could just offload a lot of decision logic and complexity around time zone handling and bucketing (that’s a big one) to TimescaleDB and let it handle that complexity for us.Continuous aggregates come into play in that we were able to roll up multiple resolutions of our sensor account data, our people count data, and make that available in a much more efficient way with little-to-no effort so far as the code that we actually had to write to deliver those efficiencies.Continuous aggregates are becoming less important for us now than they were in our previous usage, just because as Shane was talking about earlier on, we're growing to such a size, and our use cases are becoming so complex that we're wanting to move to more the ETL (extract, transform, load) type of processing out of the database. So we're not monopolizing database resources to do some of those computations and move them upstream of the database into processing pipelines and still take advantage of continuous aggregates but to a lesser degree. We're doing less of the mathematical stuff in the database, but we're still using continuous aggregates to roll up high-resolution data to lower resolutions.Advice & ResourcesBrock:If I had to recommend resources, the first would probably be theTimescale blog. It’s been historically pretty informative over the years about the internals, like what’s happening in TimescaleDB underpinning a specific feature. One that I remember specifically is explaining thevarious compression algorithms in play for different data types within PostgreSQL. Being able to distill that knowledge down to a blog article that I could consume and then assimilate into my mental model of what was going on under the covers of the technology I was using has been helpful repeatedly.The advice that I would give for building a scalable database or a strategy around that is that when designing for an analytic workload specifically, don't direct any read load to the master or in standby notes. Always use a read replica for enough reasons that we probably don't\", metadata={'title': 'How Density Manages Large Real Estate Portfolios Using TimescaleDB', 'url': 'https://www.timescale.com/blog/density-measures-large-real-estate-portfolios-using-timescaledb/'}),\n",
       " Document(page_content=\"aledb.continuous) AS\\n SELECT actibump_id,\\n timescaledb_experimental.time_bucket_ng(INTERVAL '1 month', time, 'UTC') AS bucket,\\n percentile_agg(vehicle_speed_initial) AS percentile_agg\\nFROM vehicles\\nGROUP BY actibump_id, bucketAnd this is the query that fetches the data for the graph:SELECT TIMESCALEDB_EXPERIMENTAL.TIME_BUCKET_NG(INTERVAL '1 month', bucket) AS date,\\nactibump_id,\\nAPPROX_PERCENTILE(0.85, ROLLUP(PERCENTILE_AGG)) AS p85,\\nMAX(signpost_speed_max)\\nFROM vehicles_summary_1_month\\nWHERE actibump_id in ('16060022')\\nAND bucket >= '2021-01-30 23:00:00'\\nAND bucket <= '2022-04-08 21:59:59'\\nGROUP BY date, actibump_id\\nORDER BY date ASCHere is an example of the graph:At the moment, we use PHP and Yii 2 to deploy TimescaleDB. We connect to TimescaleDB with Qlik Sense for business analytics. In Qlik Sense, you can easily connect to TimescaleDB using the PostgreSQL integration.It is especially convenient to be able to connect to the continuous aggregations for long-term data without overloading the system with too much raw data. We often use Qlik Sense to rapidly prototype graphs that we later add to EdevaLive.Advice and ResourcesThe next step for us is to come up with a good way of reducing the amount of raw data we store in TimescaleDB. We are looking at how we can integrate it with a data lake. Apart from that, we are really excited to start building even more graphs and map applications.If you are planning to store time-series data, Timescale is the way to go. It makes it easy to get started because it is “just” SQL, and at the same time, you get the important features needed to work with time-series data. I recommend you have a look, especially at continuous aggregations.Think about the whole lifecycle when you start. Will your use cases allow you to use features like compression, or do you need to think about how to\", metadata={'title': 'How Edeva Uses Continuous Aggregations and IoT to Build Smarter Cities', 'url': 'https://www.timescale.com/blog/how-edeva-uses-continuous-aggregations-and-iot-to-build-smarter-cities/'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[\"source_documents\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's print the out the result with the source document cited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_documents = responses[\"source_documents\"]\n",
    "source_content = [doc.page_content for doc in source_documents]\n",
    "source_metadata = [doc.metadata for doc in source_documents]\n",
    "\n",
    "# Construct a single string with the LLM output and the source titles and urls\n",
    "def construct_result_with_sources():\n",
    "    result = responses['result']\n",
    "    result += \"\\n\\n\"\n",
    "    result += \"Sources used:\"\n",
    "    for i in range(len(source_content)):\n",
    "        result += \"\\n\\n\"\n",
    "        result += source_metadata[i]['title']\n",
    "        result += \"\\n\\n\"\n",
    "        result += source_metadata[i]['url']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Edeva uses continuous aggregates in their smart city platform, EdevaLive. They collect large amounts of data from IoT devices, including traffic flow data from their dynamic speed bump called Actibump. Continuous aggregates allow them to roll up multiple resolutions of their sensor account data and people count data, making it available in a more efficient way. This helps them analyze and visualize the data faster, enabling them to provide valuable remote monitoring services and statistics to their customers. They also use continuous aggregates to roll up high-resolution data to lower resolutions, optimizing their data processing.\n",
       "\n",
       "Sources used:\n",
       "\n",
       "How Edeva Uses Continuous Aggregations and IoT to Build Smarter Cities\n",
       "\n",
       "https://www.timescale.com/blog/how-edeva-uses-continuous-aggregations-and-iot-to-build-smarter-cities/\n",
       "\n",
       "How Density Manages Large Real Estate Portfolios Using TimescaleDB\n",
       "\n",
       "https://www.timescale.com/blog/density-measures-large-real-estate-portfolios-using-timescaledb/\n",
       "\n",
       "How Edeva Uses Continuous Aggregations and IoT to Build Smarter Cities\n",
       "\n",
       "https://www.timescale.com/blog/how-edeva-uses-continuous-aggregations-and-iot-to-build-smarter-cities/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(construct_result_with_sources()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cite your sources functionality is useful because it can help explain unexpected responses from the model due to irrelevant but highly similar documents being retrieved from the database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "- Check out [Conversational Retrieval QA Chain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db) for how to added memory and use what you learned above in a chatbot conversation setting\n",
    "- Use [Chainlit](https://github.com/Chainlit/chainlit) to build your own LLM chatbot in python (Timescale tutorial coming soon!)\n",
    "- Learn about how pgvector finds approximate nearest neighbors in this blog post: [What Are ivfflat Indexes in pgvector and How Do They Work](https://www.timescale.com/blog/nearest-neighbor-indexes-what-are-ivfflat-indexes-in-pgvector-and-how-do-they-work/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
